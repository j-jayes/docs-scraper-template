Skip to content 

[ ](../.. "Agent Development Kit")

[ Agent Development Kit ](../.. "Agent Development Kit")

BigQuery Agent Analytics 

Initializing search 




[ adk-python  ](https://github.com/google/adk-python "adk-python") [ adk-js  ](https://github.com/google/adk-js "adk-js") [ adk-go  ](https://github.com/google/adk-go "adk-go") [ adk-java  ](https://github.com/google/adk-java "adk-java")

[ adk-python  ](https://github.com/google/adk-python "adk-python") [ adk-js  ](https://github.com/google/adk-js "adk-js") [ adk-go  ](https://github.com/google/adk-go "adk-go") [ adk-java  ](https://github.com/google/adk-java "adk-java")

  * [ Home  ](../..)

Home 
  * Build Agents  Build Agents 
    * [ Get Started  ](../../get-started/)

Get Started 
      * [ Python  ](../../get-started/python/)
      * [ TypeScript  ](../../get-started/typescript/)
      * [ Go  ](../../get-started/go/)
      * [ Java  ](../../get-started/java/)
    * [ Build your Agent  ](../../tutorials/)

Build your Agent 
      * [ Multi-tool agent  ](../../get-started/quickstart/)
      * [ Agent team  ](../../tutorials/agent-team/)
      * [ Streaming agent  ](../../get-started/streaming/)

Streaming agent 
        * [ Python  ](../../get-started/streaming/quickstart-streaming/)
        * [ Java  ](../../get-started/streaming/quickstart-streaming-java/)
      * [ Visual Builder  ](../../visual-builder/)
      * [ Coding with AI  ](../../tutorials/coding-with-ai/)
      * [ Advanced setup  ](../../get-started/installation/)
    * [ Agents  ](../../agents/)

Agents 
      * [ LLM agents  ](../../agents/llm-agents/)
      * [ Workflow agents  ](../../agents/workflow-agents/)

Workflow agents 
        * [ Sequential agents  ](../../agents/workflow-agents/sequential-agents/)
        * [ Loop agents  ](../../agents/workflow-agents/loop-agents/)
        * [ Parallel agents  ](../../agents/workflow-agents/parallel-agents/)
      * [ Custom agents  ](../../agents/custom-agents/)
      * [ Multi-agent systems  ](../../agents/multi-agents/)
      * [ Agent Config  ](../../agents/config/)
      * [ Models & Authentication  ](../../agents/models/)
    * [ Tools for Agents  ](../../tools/)

Tools for Agents 
      * [ Gemini API tools  ](../../tools/gemini-api/)

Gemini API tools 
        * [ Code Execution  ](../../tools/gemini-api/code-execution/)
        * [ Computer use  ](../../tools/gemini-api/computer-use/)
        * [ Google Search  ](../../tools/gemini-api/google-search/)
      * [ Google Cloud tools  ](../../tools/google-cloud/)

Google Cloud tools 
        * [ Apigee API Hub  ](../../tools/google-cloud/apigee-api-hub/)
        * [ Application Integration  ](../../tools/google-cloud/application-integration/)
        * [ BigQuery  ](../../tools/google-cloud/bigquery/)
        * [ Bigtable  ](../../tools/google-cloud/bigtable/)
        * [ Cloud API Registry  ](../../tools/google-cloud/api-registry/)
        * [ Code Execution with Agent Engine  ](../../tools/google-cloud/code-exec-agent-engine/)
        * [ GKE Code Executor  ](../../tools/google-cloud/gke-code-executor/)
        * [ MCP Toolbox for Databases  ](../../tools/google-cloud/mcp-toolbox-for-databases/)
        * [ RAG Engine  ](../../tools/google-cloud/vertex-ai-rag-engine/)
        * [ Spanner  ](../../tools/google-cloud/spanner/)
        * [ Vertex AI Search  ](../../tools/google-cloud/vertex-ai-search/)
      * [ Third-party tools  ](../../tools/third-party/)

Third-party tools 
        * [ Atlassian  ](../../tools/third-party/atlassian/)
        * [ GitHub  ](../../tools/third-party/github/)
        * [ GitLab  ](../../tools/third-party/gitlab/)
        * [ Hugging Face  ](../../tools/third-party/hugging-face/)
        * [ Linear  ](../../tools/third-party/linear/)
        * [ n8n  ](../../tools/third-party/n8n/)
        * [ Notion  ](../../tools/third-party/notion/)
        * [ PayPal  ](../../tools/third-party/paypal/)
        * [ Qdrant  ](../../tools/third-party/qdrant/)
        * [ Agentic UI (AG-UI)  ](../../tools/third-party/ag-ui/)
      * [ Tool limitations  ](../../tools/limitations/)
    * [ Custom Tools  ](../../tools-custom/)

Custom Tools 
      * Function tools  Function tools 
        * [ Overview  ](../../tools-custom/function-tools/)
        * [ Tool performance  ](../../tools-custom/performance/)
        * [ Action confirmations  ](../../tools-custom/confirmation/)
      * [ MCP tools  ](../../tools-custom/mcp-tools/)
      * [ OpenAPI tools  ](../../tools-custom/openapi-tools/)
      * [ Authentication  ](../../tools-custom/authentication/)
  * Run Agents  Run Agents 
    * [ Agent Runtime  ](../../runtime/)

Agent Runtime 
      * [ Runtime Config  ](../../runtime/runconfig/)
      * [ API Server  ](../../runtime/api-server/)
      * [ Resume Agents  ](../../runtime/resume/)
    * [ Deployment  ](../../deploy/)

Deployment 
      * [ Agent Engine  ](../../deploy/agent-engine/)

Agent Engine 
        * [ Standard deployment  ](../../deploy/agent-engine/deploy/)
        * [ Agent Starter Pack  ](../../deploy/agent-engine/asp/)
        * [ Test deployed agents  ](../../deploy/agent-engine/test/)
      * [ Cloud Run  ](../../deploy/cloud-run/)
      * [ GKE  ](../../deploy/gke/)
    * Observability  Observability 
      * [ Logging  ](../logging/)
      * [ Cloud Trace  ](../cloud-trace/)
      * BigQuery Agent Analytics  [ BigQuery Agent Analytics  ](./) Table of contents 
        * Use cases 
        * Prerequisites 
          * IAM permissions 
        * Use with agent 
          * Run and test agent 
        * Configuration options 
        * Schema and production setup 
          * Event types and payloads 
            * LLM interactions (plugin lifecycle) 
            * Tool usage (plugin lifecycle) 
            * Agent lifecycle & Generic Events 
            * GCS Offloading Examples (Multimodal & Large Text) 
        * Advanced analysis queries 
        * Additional resources 
      * [ AgentOps  ](../agentops/)
      * [ Arize AX  ](../arize-ax/)
      * [ Freeplay  ](../freeplay/)
      * [ MLflow  ](../mlflow/)
      * [ Monocle  ](../monocle/)
      * [ Phoenix  ](../phoenix/)
      * [ W&B; Weave  ](../weave/)
    * [ Evaluation  ](../../evaluate/)

Evaluation 
      * [ Criteria  ](../../evaluate/criteria/)
      * [ User Simulation  ](../../evaluate/user-sim/)
    * [ Safety and Security  ](../../safety/)

Safety and Security 
  * Components  Components 
    * [ Technical Overview  ](../../get-started/about/)
    * [ Context  ](../../context/)

Context 
      * [ Context caching  ](../../context/caching/)
      * [ Context compression  ](../../context/compaction/)
    * [ Sessions & Memory  ](../../sessions/)

Sessions & Memory 
      * Sessions  Sessions 
        * [ Overview  ](../../sessions/session/)
        * [ Rewind sessions  ](../../sessions/rewind/)
      * [ State  ](../../sessions/state/)
      * [ Memory  ](../../sessions/memory/)
      * [ Vertex AI Express Mode  ](../../sessions/express-mode/)
    * [ Callbacks  ](../../callbacks/)

Callbacks 
      * [ Types of callbacks  ](../../callbacks/types-of-callbacks/)
      * [ Callback patterns  ](../../callbacks/design-patterns-and-best-practices/)
    * [ Artifacts  ](../../artifacts/)

Artifacts 
    * [ Events  ](../../events/)

Events 
    * [ Apps  ](../../apps/)

Apps 
    * [ Plugins  ](../../plugins/)

Plugins 
      * [ Reflect and retry  ](../../plugins/reflect-and-retry/)
    * [ MCP  ](../../mcp/)

MCP 
    * [ A2A Protocol  ](../../a2a/)

A2A Protocol 
      * [ Introduction to A2A  ](../../a2a/intro/)
      * A2A Quickstart (Exposing)  A2A Quickstart (Exposing) 
        * [ Python  ](../../a2a/quickstart-exposing/)
        * [ Go  ](../../a2a/quickstart-exposing-go/)
      * A2A Quickstart (Consuming)  A2A Quickstart (Consuming) 
        * [ Python  ](../../a2a/quickstart-consuming/)
        * [ Go  ](../../a2a/quickstart-consuming-go/)
    * [ Bidi-streaming (live)  ](../../streaming/)

Bidi-streaming (live) 
      * Bidi-streaming development guide series  Bidi-streaming development guide series 
        * [ Part 1. Intro to streaming  ](../../streaming/dev-guide/part1/)
        * [ Part 2. Sending messages  ](../../streaming/dev-guide/part2/)
        * [ Part 3. Event handling  ](../../streaming/dev-guide/part3/)
        * [ Part 4. Run configuration  ](../../streaming/dev-guide/part4/)
        * [ Part 5. Audio, Images, and Video  ](../../streaming/dev-guide/part5/)
      * [ Streaming Tools  ](../../streaming/streaming-tools/)
      * [ Configuring Bidi-streaming behavior  ](../../streaming/configuration/)
    * Grounding  Grounding 
      * [ Understanding Google Search Grounding  ](../../grounding/google_search_grounding/)
      * [ Understanding Vertex AI Search Grounding  ](../../grounding/vertex_ai_search_grounding/)
  * Reference  Reference 
    * [ Release Notes  ](../../release-notes/)
    * [ API Reference  ](../../api-reference/)

API Reference 
      * [ Python ADK  ](../../api-reference/python/)
      * [ Typescript ADK  ](../../api-reference/typescript/)
      * [ Go ADK  ](https://pkg.go.dev/google.golang.org/adk)
      * [ Java ADK  ](../../api-reference/java/)
      * [ CLI Reference  ](../../api-reference/cli/)
      * [ Agent Config Reference  ](../../api-reference/agentconfig/)
      * [ REST API  ](../../api-reference/rest/)
    * [ Community Resources  ](../../community/)
    * [ Contributing Guide  ](../../contributing-guide/)



Table of contents 

  * Use cases 
  * Prerequisites 
    * IAM permissions 
  * Use with agent 
    * Run and test agent 
  * Configuration options 
  * Schema and production setup 
    * Event types and payloads 
      * LLM interactions (plugin lifecycle) 
      * Tool usage (plugin lifecycle) 
      * Agent lifecycle & Generic Events 
      * GCS Offloading Examples (Multimodal & Large Text) 
  * Advanced analysis queries 
  * Additional resources 



  1. [ Run Agents  ](../../runtime/)
  2. [ Observability  ](../logging/)

[ ](https://github.com/google/adk-docs/edit/main/docs/observability/bigquery-agent-analytics.md "Edit this page") [ ](https://github.com/google/adk-docs/raw/main/docs/observability/bigquery-agent-analytics.md "View source of this page")

# BigQuery Agent Analytics Plugin¶

Supported in ADKPython v1.21.0Preview

Version Requirement

Use the **_latest version_** of the ADK (version 1.21.0 or higher) to make full use of the features described in this document.

The BigQuery Agent Analytics Plugin significantly enhances the Agent Development Kit (ADK) by providing a robust solution for in-depth agent behavior analysis. Using the ADK Plugin architecture and the **BigQuery Storage Write API** , it captures and logs critical operational events directly into a Google BigQuery table, empowering you with advanced capabilities for debugging, real-time monitoring, and comprehensive offline performance evaluation.

Version 1.21.0 introduces **Hybrid Multimodal Logging** , allowing you to log large payloads (images, audio, blobs) by offloading them to Google Cloud Storage (GCS) while keeping a structured reference (`ObjectRef`) in BigQuery.

Preview release

The BigQuery Agent Analytics Plugin is in Preview release. For more information, see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages).

BigQuery Storage Write API

This feature uses **BigQuery Storage Write API** , which is a paid service. For information on costs, see the [BigQuery documentation](https://cloud.google.com/bigquery/pricing?e=48754805&hl=en#data-ingestion-pricing).

## Use cases¶

  * **Agent workflow debugging and analysis:** Capture a wide range of _plugin lifecycle events_ (LLM calls, tool usage) and _agent-yielded events_ (user input, model responses), into a well-defined schema.
  * **High-volume analysis and debugging:** Logging operations are performed asynchronously using the Storage Write API to allow high throughput and low latency.
  * **Multimodal Analysis** : Log and analyze text, images, and other modalities. Large files are offloaded to GCS, making them accessible to BigQuery ML via Object Tables.
  * **Distributed Tracing** : Built-in support for OpenTelemetry-style tracing (`trace_id`, `span_id`) to visualize agent execution flows.



The agent event data recorded varies based on the ADK event type. For more information, see Event types and payloads.

## Prerequisites¶

  * **Google Cloud Project** with the **BigQuery API** enabled.
  * **BigQuery Dataset:** Create a dataset to store logging tables before using the plugin. The plugin automatically creates the necessary events table within the dataset if the table does not exist.
  * **Google Cloud Storage Bucket (Optional):** If you plan to log multimodal content (images, audio, etc.), creating a GCS bucket is recommended for offloading large files.
  * **Authentication:**
    * **Local:** Run `gcloud auth application-default login`.
    * **Cloud:** Ensure your service account has the required permissions.



### IAM permissions¶

For the agent to work properly, the principal (e.g., service account, user account) under which the agent is running needs these Google Cloud roles: * `roles/bigquery.jobUser` at Project Level to run BigQuery queries. * `roles/bigquery.dataEditor` at Table Level to write log/event data. * **If using GCS offloading:** `roles/storage.objectCreator` and `roles/storage.objectViewer` on the target bucket.

## Use with agent¶

You use the BigQuery Agent Analytics Plugin by configuring and registering it with your ADK agent's App object. The following example shows an implementation of an agent with this plugin, including GCS offloading:

my_bq_agent/agent.py
    
    
    # my_bq_agent/agent.py
    import os
    import google.auth
    from google.adk.apps import App
    from google.adk.plugins.bigquery_agent_analytics_plugin import BigQueryAgentAnalyticsPlugin, BigQueryLoggerConfig
    from google.adk.agents import Agent
    from google.adk.models.google_llm import Gemini
    from google.adk.tools.bigquery import BigQueryToolset, BigQueryCredentialsConfig
    
    # --- Configuration ---
    PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "your-gcp-project-id")
    DATASET_ID = os.environ.get("BIG_QUERY_DATASET_ID", "your-big-query-dataset-id")
    LOCATION = os.environ.get("GOOGLE_CLOUD_LOCATION", "US") # default location is US in the plugin
    GCS_BUCKET = os.environ.get("GCS_BUCKET_NAME", "your-gcs-bucket-name") # Optional
    
    if PROJECT_ID == "your-gcp-project-id":
        raise ValueError("Please set GOOGLE_CLOUD_PROJECT or update the code.")
    
    # --- CRITICAL: Set environment variables BEFORE Gemini instantiation ---
    os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID
    os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION
    os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True'
    
    # --- Initialize the Plugin with Config ---
    bq_config = BigQueryLoggerConfig(
        enabled=True,
        gcs_bucket_name=GCS_BUCKET, # Enable GCS offloading for multimodal content
        log_multi_modal_content=True,
        max_content_length=500 * 1024, # 500 KB limit for inline text
        batch_size=1, # Default is 1 for low latency, increase for high throughput
        shutdown_timeout=10.0
    )
    
    bq_logging_plugin = BigQueryAgentAnalyticsPlugin(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        table_id="agent_events_v2", # default table name is agent_events_v2
        config=bq_config,
        location=LOCATION
    )
    
    # --- Initialize Tools and Model ---
    credentials, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
    bigquery_toolset = BigQueryToolset(
        credentials_config=BigQueryCredentialsConfig(credentials=credentials)
    )
    
    llm = Gemini(model="gemini-2.5-flash")
    
    root_agent = Agent(
        model=llm,
        name='my_bq_agent',
        instruction="You are a helpful assistant with access to BigQuery tools.",
        tools=[bigquery_toolset]
    )
    
    # --- Create the App ---
    app = App(
        name="my_bq_agent",
        root_agent=root_agent,
        plugins=[bq_logging_plugin],
    )
    

### Run and test agent¶

Test the plugin by running the agent and making a few requests through the chat interface, such as ”tell me what you can do” or "List datasets in my cloud project  “. These actions create events which are recorded in your Google Cloud project BigQuery instance. Once these events have been processed, you can view the data for them in the [BigQuery Console](https://console.cloud.google.com/bigquery), using this query
    
    
    SELECT timestamp, event_type, content 
    FROM `your-gcp-project-id.your-big-query-dataset-id.agent_events_v2`
    ORDER BY timestamp DESC
    LIMIT 20;
    

## Configuration options¶

You can customize the plugin using `BigQueryLoggerConfig`.

  * **`enabled`** (`bool`, default: `True`): To disable the plugin from logging agent data to the BigQuery table, set this parameter to False.
  * **`clustering_fields`** (`List[str]`, default: `["event_type", "agent", "user_id"]`): The fields used to cluster the BigQuery table when it is automatically created.
  * **`gcs_bucket_name`** (`Optional[str]`, default: `None`): The name of the GCS bucket to offload large content (images, blobs, large text) to. If not provided, large content may be truncated or replaced with placeholders.
  * **`connection_id`** (`Optional[str]`, default: `None`): The BigQuery connection ID (e.g., `us.my-connection`) to use as the authorizer for `ObjectRef` columns. Required for using `ObjectRef` with BigQuery ML.
  * **`max_content_length`** (`int`, default: `500 * 1024`): The maximum length (in characters) of text content to store **inline** in BigQuery before offloading to GCS (if configured) or truncating. Default is 500 KB.
  * **`batch_size`** (`int`, default: `1`): The number of events to batch before writing to BigQuery.
  * **`batch_flush_interval`** (`float`, default: `1.0`): The maximum time (in seconds) to wait before flushing a partial batch.
  * **`shutdown_timeout`** (`float`, default: `10.0`): Seconds to wait for logs to flush during shutdown.
  * **`event_allowlist`** (`Optional[List[str]]`, default: `None`): A list of event types to log. If `None`, all events are logged except those in `event_denylist`. For a comprehensive list of supported event types, refer to the Event types and payloads section.
  * **`event_denylist`** (`Optional[List[str]]`, default: `None`): A list of event types to skip logging. For a comprehensive list of supported event types, refer to the Event types and payloads section.
  * **`content_formatter`** (`Optional[Callable[[Any, str], Any]]`, default: `None`): An optional function to format event content before logging.
  * **`log_multi_modal_content`** (`bool`, default: `True`): Whether to log detailed content parts (including GCS references).



The following code sample shows how to define a configuration for the BigQuery Agent Analytics plugin:
    
    
    import json
    import re
    
    from google.adk.plugins.bigquery_agent_analytics_plugin import BigQueryLoggerConfig
    
    def redact_dollar_amounts(event_content: Any) -> str:
        """
        Custom formatter to redact dollar amounts (e.g., $600, $12.50)
        and ensure JSON output if the input is a dict.
        """
        text_content = ""
        if isinstance(event_content, dict):
            text_content = json.dumps(event_content)
        else:
            text_content = str(event_content)
    
        # Regex to find dollar amounts: $ followed by digits, optionally with commas or decimals.
        # Examples: $600, $1,200.50, $0.99
        redacted_content = re.sub(r'\$\d+(?:,\d{3})*(?:\.\d+)?', 'xxx', text_content)
    
        return redacted_content
    
    config = BigQueryLoggerConfig(
        enabled=True,
        event_allowlist=["LLM_REQUEST", "LLM_RESPONSE"], # Only log these events
        # event_denylist=["TOOL_STARTING"], # Skip these events
        shutdown_timeout=10.0, # Wait up to 10s for logs to flush on exit
        client_close_timeout=2.0, # Wait up to 2s for BQ client to close
        max_content_length=500, # Truncate content to 500 chars
        content_formatter=redact_dollar_amounts, # Redact the dollar amounts in the logging content
    
    )
    
    plugin = BigQueryAgentAnalyticsPlugin(..., config=config)
    

## Schema and production setup¶

The plugin automatically creates the table if it does not exist. However, for production, we recommend creating the table manually using the following DDL, which utilizes the **JSON** type for flexibility and **REPEATED RECORD** s for multimodal content.

**Recommended DDL:**
    
    
    CREATE TABLE `your-gcp-project-id.adk_agent_logs.agent_events_v2`
    (
      timestamp TIMESTAMP NOT NULL OPTIONS(description="The UTC time at which the event was logged."),
      event_type STRING OPTIONS(description="Indicates the type of event being logged (e.g., 'LLM_REQUEST', 'TOOL_COMPLETED')."),
      agent STRING OPTIONS(description="The name of the ADK agent or author associated with the event."),
      session_id STRING OPTIONS(description="A unique identifier to group events within a single conversation or user session."),
      invocation_id STRING OPTIONS(description="A unique identifier for each individual agent execution or turn within a session."),
      user_id STRING OPTIONS(description="The identifier of the user associated with the current session."),
      trace_id STRING OPTIONS(description="OpenTelemetry trace ID for distributed tracing."),
      span_id STRING OPTIONS(description="OpenTelemetry span ID for this specific operation."),
      parent_span_id STRING OPTIONS(description="OpenTelemetry parent span ID to reconstruct hierarchy."),
      content JSON OPTIONS(description="The event-specific data (payload) stored as JSON."),
      content_parts ARRAY<STRUCT<
        mime_type STRING,
        uri STRING,
        object_ref STRUCT<
          uri STRING,
          version STRING,
          authorizer STRING,
          details JSON
        >,
        text STRING,
        part_index INT64,
        part_attributes STRING,
        storage_mode STRING
      >> OPTIONS(description="Detailed content parts for multi-modal data."),
      attributes JSON OPTIONS(description="Arbitrary key-value pairs for additional metadata."),
      latency_ms JSON OPTIONS(description="Latency measurements (e.g., total_ms)."),
      status STRING OPTIONS(description="The outcome of the event, typically 'OK' or 'ERROR'."),
      error_message STRING OPTIONS(description="Populated if an error occurs."),
      is_truncated BOOLEAN OPTIONS(description="Flag indicates if content was truncated.")
    )
    PARTITION BY DATE(timestamp)
    CLUSTER BY event_type, agent, user_id;
    

### Event types and payloads¶

The `content` column now contains a **JSON** object specific to the `event_type`. The `content_parts` column provides a structured view of the content, especially useful for images or offloaded data.

Content Truncation

  * Variable content fields are truncated to `max_content_length` (configured in `BigQueryLoggerConfig`, default 500KB).
  * If `gcs_bucket_name` is configured, large content is offloaded to GCS instead of being truncated, and a reference is stored in `content_parts.object_ref`.



#### LLM interactions (plugin lifecycle)¶

These events track the raw requests sent to and responses received from the LLM.

**Event Type** | **Content (JSON) Structure** | **Attributes (JSON)** | **Example Content (Simplified)**  
---|---|---|---  
      
    
    LLM_REQUEST

| 
    
    
    {
      "prompt": [
        {"role": "user", "content": "..."}
      ],
      "system_prompt": "..."
    }
    

| 
    
    
    {
      "tools": ["tool_a", "tool_b"],
      "llm_config": {"temperature": 0.5}
    }
    

| 
    
    
    {
      "prompt": [
        {"role": "user", "content": "What is the capital of France?"}
      ],
      "system_prompt": "You are a helpful geography assistant."
    }
      
      
    
    LLM_RESPONSE

| 
    
    
    {
      "response": "...",
      "usage": {...}
    }
    

| 
    
    
    {}

| 
    
    
    {
      "response": "The capital of France is Paris.",
      "usage": {
        "prompt": 15,
        "completion": 7,
        "total": 22
      }
    }
      
      
    
    LLM_ERROR

| 
    
    
    null

| 
    
    
    {}

| 
    
    
    null (See error_message column)  
  
#### Tool usage (plugin lifecycle)¶

These events track the execution of tools by the agent.

**Event Type** | **Content (JSON) Structure** | **Attributes (JSON)** | **Example Content**  
---|---|---|---  
      
    
    TOOL_STARTING

| 
    
    
    {
      "tool": "...",
      "args": {...}
    }
    

| 
    
    
    {}

| 
    
    
    {"tool": "list_datasets", "args": {"project_id": "my-project"}}
      
      
    
    TOOL_COMPLETED

| 
    
    
    {
      "tool": "...",
      "result": "..."
    }
    

| 
    
    
    {}

| 
    
    
    {"tool": "list_datasets", "result": ["ds1", "ds2"]}
      
      
    
    TOOL_ERROR

| 
    
    
    {
      "tool": "...",
      "args": {...}
    }
    

| 
    
    
    {}

| 
    
    
    {"tool": "list_datasets", "args": {}}
      
  
#### Agent lifecycle & Generic Events¶

**Event Type** | **Content (JSON) Structure**  
---|---  
      
    
    INVOCATION_STARTING

| 
    
    
    {}  
      
    
    INVOCATION_COMPLETED

| 
    
    
    {}  
      
    
    AGENT_STARTING

| 
    
    
    "You are a helpful agent..."  
      
    
    AGENT_COMPLETED

| 
    
    
    {}  
      
    
    USER_MESSAGE_RECEIVED

| 
    
    
    {"text_summary": "Help me book a flight."}  
  
#### GCS Offloading Examples (Multimodal & Large Text)¶

When `gcs_bucket_name` is configured, large text and multimodal content (images, audio, etc.) are automatically offloaded to GCS. The `content` column will contain a summary or placeholder, while `content_parts` contains the `object_ref` pointing to the GCS URI.

**Offloaded Text Example**
    
    
    {
      "event_type": "LLM_REQUEST",
      "content_parts": [
        {
          "part_index": 1,
          "mime_type": "text/plain",
          "storage_mode": "GCS_REFERENCE",
          "text": "AAAA... [OFFLOADED]",
          "object_ref": {
            "uri": "gs://haiyuan-adk-debug-verification-1765319132/2025-12-10/e-f9545d6d/ae5235e6_p1.txt",
            "authorizer": "us.bqml_connection",
            "details": {"gcs_metadata": {"content_type": "text/plain"}}
          }
        }
      ]
    }
    

**Offloaded Image Example**
    
    
    {
      "event_type": "LLM_REQUEST",
      "content_parts": [
        {
          "part_index": 2,
          "mime_type": "image/png",
          "storage_mode": "GCS_REFERENCE",
          "text": "[MEDIA OFFLOADED]",
          "object_ref": {
            "uri": "gs://haiyuan-adk-debug-verification-1765319132/2025-12-10/e-f9545d6d/ae5235e6_p2.png",
            "authorizer": "us.bqml_connection",
            "details": {"gcs_metadata": {"content_type": "image/png"}}
          }
        }
      ]
    }
    

**Querying Offloaded Content (Get Signed URLs)**
    
    
    SELECT
      timestamp,
      event_type,
      part.mime_type,
      part.storage_mode,
      part.object_ref.uri AS gcs_uri,
      -- Generate a signed URL to read the content directly (requires connection_id configuration)
      STRING(OBJ.GET_ACCESS_URL(part.object_ref, 'r').access_urls.read_url) AS signed_url
    FROM `your-gcp-project-id.your-dataset-id.agent_events_v2`,
    UNNEST(content_parts) AS part
    WHERE part.storage_mode = 'GCS_REFERENCE'
    ORDER BY timestamp DESC
    LIMIT 10;
    

## Advanced analysis queries¶

**Trace a specific conversation turn using trace_id**
    
    
    SELECT timestamp, event_type, agent, JSON_VALUE(content, '$.response') as summary
    FROM `your-gcp-project-id.your-dataset-id.agent_events_v2`
    WHERE trace_id = 'your-trace-id'
    ORDER BY timestamp ASC;
    

**Token usage analysis (accessing JSON fields)**
    
    
    SELECT
      AVG(CAST(JSON_VALUE(content, '$.usage.total') AS INT64)) as avg_tokens
    FROM `your-gcp-project-id.your-dataset-id.agent_events_v2`
    WHERE event_type = 'LLM_RESPONSE';
    

**Querying Multimodal Content (using content_parts and ObjectRef)**
    
    
    SELECT
      timestamp,
      part.mime_type,
      part.object_ref.uri as gcs_uri
    FROM `your-gcp-project-id.your-dataset-id.agent_events_v2`,
    UNNEST(content_parts) as part
    WHERE part.mime_type LIKE 'image/%'
    ORDER BY timestamp DESC;
    

**Analyze Multimodal Content with BigQuery Remote Model (Gemini)**
    
    
    SELECT
      logs.session_id,
      -- Get a signed URL for the image
      STRING(OBJ.GET_ACCESS_URL(parts.object_ref, "r").access_urls.read_url) as signed_url,
      -- Analyze the image using a remote model (e.g., gemini-pro-vision)
      AI.GENERATE(
        ('Describe this image briefly. What company logo?', parts.object_ref)
      ) AS generated_result
    FROM
      `your-gcp-project-id.your-dataset-id.agent_events_v2` logs,
      UNNEST(logs.content_parts) AS parts
    WHERE
      parts.mime_type LIKE 'image/%'
    ORDER BY logs.timestamp DESC
    LIMIT 1;
    

**Latency Analysis (LLM & Tools)**
    
    
    SELECT
      event_type,
      AVG(CAST(JSON_VALUE(latency_ms, '$.total_ms') AS INT64)) as avg_latency_ms
    FROM `your-gcp-project-id.your-dataset-id.agent_events_v2`
    WHERE event_type IN ('LLM_RESPONSE', 'TOOL_COMPLETED')
    GROUP BY event_type;
    

**Span Hierarchy & Duration Analysis**
    
    
    SELECT
      span_id,
      parent_span_id,
      event_type,
      timestamp,
      -- Extract duration from latency_ms for completed operations
      CAST(JSON_VALUE(latency_ms, '$.total_ms') AS INT64) as duration_ms,
      -- Identify the specific tool or operation
      COALESCE(
        JSON_VALUE(content, '$.tool'), 
        'LLM_CALL'
      ) as operation
    FROM `your-gcp-project-id.your-dataset-id.agent_events_v2`
    WHERE trace_id = 'your-trace-id'
      AND event_type IN ('LLM_RESPONSE', 'TOOL_COMPLETED')
    ORDER BY timestamp ASC;
    

## Additional resources¶

  * [BigQuery Storage Write API](https://cloud.google.com/bigquery/docs/write-api)
  * [Introduction to Object Tables](https://cloud.google.com/bigquery/docs/object-tables-intro)



Back to top  [ Previous  Cloud Trace  ](../cloud-trace/) [ Next  AgentOps  ](../agentops/)

Copyright Google 2025  |  [Terms](//policies.google.com/terms)  |  [Privacy](//policies.google.com/privacy)  |  Manage cookies

Made with [ Material for MkDocs ](https://squidfunk.github.io/mkdocs-material/)

#### Cookie consent

We use cookies to recognize repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find the information they need. With your consent, you're helping us to make our documentation better.

  * Google Analytics 
  * GitHub 



Accept Manage settings
