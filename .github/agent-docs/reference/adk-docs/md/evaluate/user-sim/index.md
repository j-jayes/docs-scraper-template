Skip to content 

[ ](../.. "Agent Development Kit")

[ Agent Development Kit ](../.. "Agent Development Kit")

User Simulation 

Initializing search 




[ adk-python  ](https://github.com/google/adk-python "adk-python") [ adk-js  ](https://github.com/google/adk-js "adk-js") [ adk-go  ](https://github.com/google/adk-go "adk-go") [ adk-java  ](https://github.com/google/adk-java "adk-java")

[ adk-python  ](https://github.com/google/adk-python "adk-python") [ adk-js  ](https://github.com/google/adk-js "adk-js") [ adk-go  ](https://github.com/google/adk-go "adk-go") [ adk-java  ](https://github.com/google/adk-java "adk-java")

  * [ Home  ](../..)

Home 
  * Build Agents  Build Agents 
    * [ Get Started  ](../../get-started/)

Get Started 
      * [ Python  ](../../get-started/python/)
      * [ TypeScript  ](../../get-started/typescript/)
      * [ Go  ](../../get-started/go/)
      * [ Java  ](../../get-started/java/)
    * [ Build your Agent  ](../../tutorials/)

Build your Agent 
      * [ Multi-tool agent  ](../../get-started/quickstart/)
      * [ Agent team  ](../../tutorials/agent-team/)
      * [ Streaming agent  ](../../get-started/streaming/)

Streaming agent 
        * [ Python  ](../../get-started/streaming/quickstart-streaming/)
        * [ Java  ](../../get-started/streaming/quickstart-streaming-java/)
      * [ Visual Builder  ](../../visual-builder/)
      * [ Coding with AI  ](../../tutorials/coding-with-ai/)
      * [ Advanced setup  ](../../get-started/installation/)
    * [ Agents  ](../../agents/)

Agents 
      * [ LLM agents  ](../../agents/llm-agents/)
      * [ Workflow agents  ](../../agents/workflow-agents/)

Workflow agents 
        * [ Sequential agents  ](../../agents/workflow-agents/sequential-agents/)
        * [ Loop agents  ](../../agents/workflow-agents/loop-agents/)
        * [ Parallel agents  ](../../agents/workflow-agents/parallel-agents/)
      * [ Custom agents  ](../../agents/custom-agents/)
      * [ Multi-agent systems  ](../../agents/multi-agents/)
      * [ Agent Config  ](../../agents/config/)
      * [ Models & Authentication  ](../../agents/models/)
    * [ Tools for Agents  ](../../tools/)

Tools for Agents 
      * [ Gemini API tools  ](../../tools/gemini-api/)

Gemini API tools 
        * [ Code Execution  ](../../tools/gemini-api/code-execution/)
        * [ Computer use  ](../../tools/gemini-api/computer-use/)
        * [ Google Search  ](../../tools/gemini-api/google-search/)
      * [ Google Cloud tools  ](../../tools/google-cloud/)

Google Cloud tools 
        * [ Apigee API Hub  ](../../tools/google-cloud/apigee-api-hub/)
        * [ Application Integration  ](../../tools/google-cloud/application-integration/)
        * [ BigQuery  ](../../tools/google-cloud/bigquery/)
        * [ Bigtable  ](../../tools/google-cloud/bigtable/)
        * [ Cloud API Registry  ](../../tools/google-cloud/api-registry/)
        * [ Code Execution with Agent Engine  ](../../tools/google-cloud/code-exec-agent-engine/)
        * [ GKE Code Executor  ](../../tools/google-cloud/gke-code-executor/)
        * [ MCP Toolbox for Databases  ](../../tools/google-cloud/mcp-toolbox-for-databases/)
        * [ RAG Engine  ](../../tools/google-cloud/vertex-ai-rag-engine/)
        * [ Spanner  ](../../tools/google-cloud/spanner/)
        * [ Vertex AI Search  ](../../tools/google-cloud/vertex-ai-search/)
      * [ Third-party tools  ](../../tools/third-party/)

Third-party tools 
        * [ Atlassian  ](../../tools/third-party/atlassian/)
        * [ GitHub  ](../../tools/third-party/github/)
        * [ GitLab  ](../../tools/third-party/gitlab/)
        * [ Hugging Face  ](../../tools/third-party/hugging-face/)
        * [ Linear  ](../../tools/third-party/linear/)
        * [ n8n  ](../../tools/third-party/n8n/)
        * [ Notion  ](../../tools/third-party/notion/)
        * [ PayPal  ](../../tools/third-party/paypal/)
        * [ Qdrant  ](../../tools/third-party/qdrant/)
        * [ Agentic UI (AG-UI)  ](../../tools/third-party/ag-ui/)
      * [ Tool limitations  ](../../tools/limitations/)
    * [ Custom Tools  ](../../tools-custom/)

Custom Tools 
      * Function tools  Function tools 
        * [ Overview  ](../../tools-custom/function-tools/)
        * [ Tool performance  ](../../tools-custom/performance/)
        * [ Action confirmations  ](../../tools-custom/confirmation/)
      * [ MCP tools  ](../../tools-custom/mcp-tools/)
      * [ OpenAPI tools  ](../../tools-custom/openapi-tools/)
      * [ Authentication  ](../../tools-custom/authentication/)
  * Run Agents  Run Agents 
    * [ Agent Runtime  ](../../runtime/)

Agent Runtime 
      * [ Runtime Config  ](../../runtime/runconfig/)
      * [ API Server  ](../../runtime/api-server/)
      * [ Resume Agents  ](../../runtime/resume/)
    * [ Deployment  ](../../deploy/)

Deployment 
      * [ Agent Engine  ](../../deploy/agent-engine/)

Agent Engine 
        * [ Standard deployment  ](../../deploy/agent-engine/deploy/)
        * [ Agent Starter Pack  ](../../deploy/agent-engine/asp/)
        * [ Test deployed agents  ](../../deploy/agent-engine/test/)
      * [ Cloud Run  ](../../deploy/cloud-run/)
      * [ GKE  ](../../deploy/gke/)
    * Observability  Observability 
      * [ Logging  ](../../observability/logging/)
      * [ Cloud Trace  ](../../observability/cloud-trace/)
      * [ BigQuery Agent Analytics  ](../../observability/bigquery-agent-analytics/)
      * [ AgentOps  ](../../observability/agentops/)
      * [ Arize AX  ](../../observability/arize-ax/)
      * [ Freeplay  ](../../observability/freeplay/)
      * [ MLflow  ](../../observability/mlflow/)
      * [ Monocle  ](../../observability/monocle/)
      * [ Phoenix  ](../../observability/phoenix/)
      * [ W&B; Weave  ](../../observability/weave/)
    * [ Evaluation  ](../)

Evaluation 
      * [ Criteria  ](../criteria/)
      * User Simulation  [ User Simulation  ](./) Table of contents 
        * Example: Evaluating the hello_world agent with conversation scenarios 
        * User simulator configuration 
    * [ Safety and Security  ](../../safety/)

Safety and Security 
  * Components  Components 
    * [ Technical Overview  ](../../get-started/about/)
    * [ Context  ](../../context/)

Context 
      * [ Context caching  ](../../context/caching/)
      * [ Context compression  ](../../context/compaction/)
    * [ Sessions & Memory  ](../../sessions/)

Sessions & Memory 
      * Sessions  Sessions 
        * [ Overview  ](../../sessions/session/)
        * [ Rewind sessions  ](../../sessions/rewind/)
      * [ State  ](../../sessions/state/)
      * [ Memory  ](../../sessions/memory/)
      * [ Vertex AI Express Mode  ](../../sessions/express-mode/)
    * [ Callbacks  ](../../callbacks/)

Callbacks 
      * [ Types of callbacks  ](../../callbacks/types-of-callbacks/)
      * [ Callback patterns  ](../../callbacks/design-patterns-and-best-practices/)
    * [ Artifacts  ](../../artifacts/)

Artifacts 
    * [ Events  ](../../events/)

Events 
    * [ Apps  ](../../apps/)

Apps 
    * [ Plugins  ](../../plugins/)

Plugins 
      * [ Reflect and retry  ](../../plugins/reflect-and-retry/)
    * [ MCP  ](../../mcp/)

MCP 
    * [ A2A Protocol  ](../../a2a/)

A2A Protocol 
      * [ Introduction to A2A  ](../../a2a/intro/)
      * A2A Quickstart (Exposing)  A2A Quickstart (Exposing) 
        * [ Python  ](../../a2a/quickstart-exposing/)
        * [ Go  ](../../a2a/quickstart-exposing-go/)
      * A2A Quickstart (Consuming)  A2A Quickstart (Consuming) 
        * [ Python  ](../../a2a/quickstart-consuming/)
        * [ Go  ](../../a2a/quickstart-consuming-go/)
    * [ Bidi-streaming (live)  ](../../streaming/)

Bidi-streaming (live) 
      * Bidi-streaming development guide series  Bidi-streaming development guide series 
        * [ Part 1. Intro to streaming  ](../../streaming/dev-guide/part1/)
        * [ Part 2. Sending messages  ](../../streaming/dev-guide/part2/)
        * [ Part 3. Event handling  ](../../streaming/dev-guide/part3/)
        * [ Part 4. Run configuration  ](../../streaming/dev-guide/part4/)
        * [ Part 5. Audio, Images, and Video  ](../../streaming/dev-guide/part5/)
      * [ Streaming Tools  ](../../streaming/streaming-tools/)
      * [ Configuring Bidi-streaming behavior  ](../../streaming/configuration/)
    * Grounding  Grounding 
      * [ Understanding Google Search Grounding  ](../../grounding/google_search_grounding/)
      * [ Understanding Vertex AI Search Grounding  ](../../grounding/vertex_ai_search_grounding/)
  * Reference  Reference 
    * [ Release Notes  ](../../release-notes/)
    * [ API Reference  ](../../api-reference/)

API Reference 
      * [ Python ADK  ](../../api-reference/python/)
      * [ Typescript ADK  ](../../api-reference/typescript/)
      * [ Go ADK  ](https://pkg.go.dev/google.golang.org/adk)
      * [ Java ADK  ](../../api-reference/java/)
      * [ CLI Reference  ](../../api-reference/cli/)
      * [ Agent Config Reference  ](../../api-reference/agentconfig/)
      * [ REST API  ](../../api-reference/rest/)
    * [ Community Resources  ](../../community/)
    * [ Contributing Guide  ](../../contributing-guide/)



Table of contents 

  * Example: Evaluating the hello_world agent with conversation scenarios 
  * User simulator configuration 



  1. [ Run Agents  ](../../runtime/)
  2. [ Evaluation  ](../)

[ ](https://github.com/google/adk-docs/edit/main/docs/evaluate/user-sim.md "Edit this page") [ ](https://github.com/google/adk-docs/raw/main/docs/evaluate/user-sim.md "View source of this page")

# User Simulation¶

Supported in ADKPython v1.18.0

When evaluating conversational agents, it is not always practical to use a fixed set of user prompts, as the conversation can proceed in unexpected ways. For example, if the agent needs the user to supply two values to perform a task, it may ask for those values one at a time or both at once. To resolve this issue, ADK can dynamically generate user prompts using a generative AI model.

To use this feature, you must specify a [`ConversationScenario`](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/conversation_scenarios.py) which dictates the user's goals in their conversation with the agent. A sample conversation scenario for the [`hello_world`](https://github.com/google/adk-python/tree/main/contributing/samples/hello_world) agent is shown below:
    
    
    {
      "starting_prompt": "What can you do for me?",
      "conversation_plan": "Ask the agent to roll a 20-sided die. After you get the result, ask the agent to check if it is prime."
    }
    

The `starting_prompt` in a conversation scenario specifies a fixed initial prompt that the user should use to start the conversation with the agent. Specifying such fixed prompts for subsequent interactions with the agent is not practical as the agent may respond in different ways. Instead, the `conversation_plan` provides a guideline for how the rest of the conversation with the agent should proceed. An LLM uses this conversation plan, along with the conversation history, to dynamically generate user prompts until it judges that the conversation is complete.

Try it in Colab

Test this entire workflow yourself in an interactive notebook on [Simulating User Conversations to Dynamically Evaluate ADK Agents](https://github.com/google/adk-samples/blob/main/python/notebooks/evaluation/user_simulation_in_adk_evals.ipynb). You'll define a conversation scenario, run a "dry run" to check the dialogue, and then perform a full evaluation to score the agent's responses.

## Example: Evaluating the [`hello_world`](https://github.com/google/adk-python/tree/main/contributing/samples/hello_world) agent with conversation scenarios¶

To add evaluation cases containing conversation scenarios to a new or existing [`EvalSet`](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_set.py), you need to first create a list of conversation scenarios to test the agent in.

Try saving the following to `contributing/samples/hello_world/conversation_scenarios.json`:
    
    
    {
      "scenarios": [
        {
          "starting_prompt": "What can you do for me?",
          "conversation_plan": "Ask the agent to roll a 20-sided die. After you get the result, ask the agent to check if it is prime."
        },
        {
          "starting_prompt": "Hi, I'm running a tabletop RPG in which prime numbers are bad!",
          "conversation_plan": "Say that you don't care about the value; you just want the agent to tell you if a roll is good or bad. Once the agent agrees, ask it to roll a 6-sided die. Finally, ask the agent to do the same with 2 20-sided dice."
        }
      ]
    }
    

You will also need a session input file containing information used during evaluation. Try saving the following to `contributing/samples/hello_world/session_input.json`:
    
    
    {
      "app_name": "hello_world",
      "user_id": "user"
    }
    

Then, you can add the conversation scenarios to an `EvalSet`:
    
    
    # (optional) create a new EvalSet
    adk eval_set create \
      contributing/samples/hello_world \
      eval_set_with_scenarios
    
    # add conversation scenarios to the EvalSet as new eval cases
    adk eval_set add_eval_case \
      contributing/samples/hello_world \
      eval_set_with_scenarios \
      --scenarios_file contributing/samples/hello_world/conversation_scenarios.json \
      --session_input_file contributing/samples/hello_world/session_input.json
    

By default, ADK runs evaluations with metrics that require the agent's expected response to be specified. Since that is not the case for a dynamic conversation scenario, we will use an [`EvalConfig`](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_config.py) with some alternate supported metrics.

Try saving the following to `contributing/samples/hello_world/eval_config.json`:
    
    
    {
      "criteria": {
        "hallucinations_v1": {
          "threshold": 0.5,
          "evaluate_intermediate_nl_responses": true
        },
        "safety_v1": {
          "threshold": 0.8
        }
      }
    }
    

Finally, you can use the `adk eval` command to run the evaluation:
    
    
    adk eval \
        contributing/samples/hello_world \
        --config_file_path contributing/samples/hello_world/eval_config.json \
        eval_set_with_scenarios \
        --print_detailed_results
    

## User simulator configuration¶

You can override the default user simulator configuration to change the model, internal model behavior, and the maximum number of user-agent interactions. The below `EvalConfig` shows the default user simulator configuration:
    
    
    {
      "criteria": {
        # same as before
      },
      "user_simulator_config": {
        "model": "gemini-2.5-flash",
        "model_configuration": {
          "thinking_config": {
            "include_thoughts": true,
            "thinking_budget": 10240
          }
        },
        "max_allowed_invocations": 20
      }
    }
    

  * `model`: The model backing the user simulator.
  * `model_configuration`: A [`GenerateContentConfig`](https://github.com/googleapis/python-genai/blob/6196b1b4251007e33661bb5d7dc27bafee3feefe/google/genai/types.py#L4295) which controls the model behavior.
  * `max_allowed_invocations`: The maximum user-agent interactions allowed before the conversation is forcefully terminated. This should be set to be greater than the longest reasonable user-agent interaction in your `EvalSet`.



Back to top  [ Previous  Criteria  ](../criteria/) [ Next  Safety and Security for AI Agents  ](../../safety/)

Copyright Google 2025  |  [Terms](//policies.google.com/terms)  |  [Privacy](//policies.google.com/privacy)  |  Manage cookies

Made with [ Material for MkDocs ](https://squidfunk.github.io/mkdocs-material/)

#### Cookie consent

We use cookies to recognize repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find the information they need. With your consent, you're helping us to make our documentation better.

  * Google Analytics 
  * GitHub 



Accept Manage settings
